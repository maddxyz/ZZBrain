\section{ZZBrain}

\subsection{Intoduction}
Dans cette section on va présenter les différentes fonctionnalités de la bibliothèque.
On a décidé de partager cette dernière en plusieurs parties différentes, la première contient tout ce qui est calculs matriciels mais adapté à notre structure presentée ci-dessus.
La deuxième partie contient les fonctions principales du réseau de neurones, et une dernière partie pour d'autres fonctions comme la Sigmoid.

\subsection{Les fonctions matricielles}

\subsubsection{Organisation du code source}

\begin{description}
\item[matrix.cpp] : La bibliothèque des fonctions matricielles.
\item[matrix.h] : Fichier contenant les prototypes des fonctions de la bibliothèque.
\end{description}
\subsubsection{Liste des fonctions}

\paragraph{Multiplication matrice matrice}
\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}
double * mult(double* M1, double* M2, int n, int m, int k)
\end{minted}

Cette fonction fait la multiplication d'une matrice \textbf{M1(n x m)} par une deuxième matrice \textbf{M2(m x k)} et qui renvoie le résultat comme pointeur vers une troisième matrice allouée par la fonction.

\paragraph{Multiplication scalaire matrice}

\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}
double * mult(double* M, double cst, int n, int m)
\end{minted}

En utilisant la notion de la surcharge en C++ on a fait une deuxième fonction pour la multiplication mais cette fois-ci d'une matrice \textbf{M(n x m)} par une constante \textbf{cst} et qui renvoie le résultat comme pointeur vers une troisième matrice allouée par la fonction.


\paragraph{Multiplication élément par élément}
\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}
double * eltMult(double * M1, double * M2, int n, int m)
\end{minted}

Cette fonction fait la multiplication élément par élément d'une matrice \textbf{M1(n x m)} par une deuxième matrice \textbf{M2(n x m)} et qui renvoie le résultat comme pointeur vers une troisième matrice allouée par la fonction.

\paragraph{Somme des deux matrices}
\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}
double * sum(double* M1, double* M2, int n, int m)
\end{minted}

Cette fonction fait la somme des deux matrice \textbf{M1(n x m)} et \textbf{M2(n x m)} et qui renvoie le résultat comme pointeur vers une matrice allouée par la fonction.

\paragraph{Différence des deux matrices}
\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}
double * diff(double * M1, double *M2, int n, int m)
\end{minted}

Cette fonction fait la différence entre des deux matrice \textbf{M1(n x m)} et \textbf{M2(n x m)} et qui renvoie le résultat comme pointeur vers une matrice allouée par la fonction.


\paragraph{Transposée d'une matrice}
\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}
double * trans(double* M, int n, int m)
\end{minted}

Cette fonction fait le transpose d'une matrice \textbf{M(n x m)} et qui renvoie le résultat comme pointeur vers une matrice allouée par la fonction.

\paragraph{Vecteur des uns}
\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}
double * ones(int n)
\end{minted}

Cette fonction génère un vecteur de taille \textbf{n} qui ne contient que \textbf{des uns} et qui renvoie un pointeur vers ce dernier.


\paragraph{Fonction d'affichage}
\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}

void print(double * M, int n, int m)
\end{minted}

Cette fonction permet d'afficher une matrice \textbf{M(n x m)} à la sortie standard. Elle est principalement utilisé lors du débogage.

\subsection{Les fonctions du réseau de neurones}

\subsubsection{Organisation du code source}

\begin{description}
\item[ZZNetwok.cpp] : La bibliothèque des fonctions principales du réseau.
\item[ZZNetwok.h] : Fichier contenant les prototypes des fonctions de la bibliothèque.
\end{description}
\subsubsection{Liste des fonctions}

\paragraph{Le constructeur}
\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}
ZZNetwork(int sizes[],int nbLayers, int setSize, double **input, double **output, double lambda=0)
\end{minted}
Le constructeur du réseau, On lui passe comme parametres :
	\begin{description}
		\item[{int sizes[]}] : Un tableau des entiers qui contient la taille (nombre des noeuds) de chaque couche.
        \item[int nbLayers] : Le nombre des couches.
        \item[int setSize] : Le nombre des éléments de la dataset utilisé pour l'apprentissage .
        \item[double **input] : La dataset utilisé pour l'apprentissage.
        \item[double **output] : Les réponses (correctes) liée au dataset \textit{**input}, permet de valider les choix fait par le réseau.
        \item[lambda=0] : Le parametre de la régularisation (utilisé pour éviter l'overfitting) par défaut il a une valeur nulle (pas de régularisation).
	\end{description}

Ses fonctionnalites :
\begin{itemize}
\item Initialisation des différentes variables du réseau par les valeurs passées en paramètre.
\item Allocation des différentes parties de la structure du réseau.
\item Initialisation des poids dans un intervalle \textbf{[-r,r]} bien precis . cet intervalle n'est adapté que lorsqu'on utilise la fonction \textit{sigmoid} , où r est un nombre réel qu'on le calcule de la facon suivant\footnote{https://stats.stackexchange.com/questions/47590/what-are-good-initial-weights-in-a-neural-network} :
\begin{equation}
     r = 4\sqrt{\frac{6}{network[i].nbNodesLayer1 + 1 + network[i].nbNodesLayer2}}
\end{equation}
Si on décide par la suite d'utiliser une autre fonction cet intervalle devra être repensé.
\end{itemize}

\paragraph{La prediction / forward propagation }
\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}

double * predict(double *input)
\end{minted}

\begin{description}
    \item[double *input] : Un tableau de réels (idéalement normalisés).
\end{description}

La fonction renvoie un vecteur contenant si oui ou non la donnée en entrée appartient à la classe k.
Pour celà nous implémentons la forward propagation comme suit :

Pour chaque couche du réseau on calcule un vecteur $z$ et on applique la fonction sigmoid sur ce dernier pour avoir le vecteur final $a$ .
\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}
memcpy(a, input, network[0].nbNodesLayer1 * sizeof(double));
for (int i = 0; i < nbLayers - 1; i++) {
		/* Add the bias node */
		memmove(a + 1, a, network[i].nbNodesLayer1 * sizeof(double));
		a[0] = 1.0;
		z = mult(network[i].weights, a, network[i].nbNodesLayer2, network[i].nbNodesLayer1 + 1, 1);
		a = sigmoid(z, network[i].nbNodesLayer2, 1);
		delete[] z;
}
\end{minted}

\paragraph{La Retropropagation }
\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}

double ***backPropagation()
\end{minted}

L'implementation de l'algorithme de la retropropagation expliqué en haut, On obtient le gradient sous forme d'un tableau tri-dimensionnel \textbf{D} comme sortie. Cette fonction n'as besoin d'aucune entrée puisque qu'elle utilise les attributs du réseaux.

\begin{itemize}
\item Allocation des différentes tableaux et vecteur utilisés pour les calculs intermédiaire.
\item Application de la forward propagation pour obtenir les vecteurs \textbf{a} intermédiaire pour chaque couche.
\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}
memcpy(a[0], input[i], network[0].nbNodesLayer1 * sizeof(double));
for (int l = 1; l < nbLayers; l++) {
	memmove(a[l - 1] + 1, a[l - 1], network[l - 1].nbNodesLayer1 * sizeof(double));
	a[l - 1][0] = 1;
	z[l] = mult(network[l - 1].weights, a[l - 1], network[l - 1].nbNodesLayer2,
	 			network[l - 1].nbNodesLayer1 + 1, 1);
	a[l] = sigmoid(z[l], network[l - 1].nbNodesLayer2, 1);
}
\end{minted}
\item Calcul des differents $\delta$ :
\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}
err[nbLayers - 1] = diff(a[nbLayers - 1], output[i], network[nbLayers - 2].nbNodesLayer2, 1);

for(int l = nbLayers - 2; l > 0 ; l--){

	tr = trans(network[l].weights, network[l].nbNodesLayer2,
				network[l].nbNodesLayer1 + 1);
	theta_delta = mult(tr, err[l + 1], network[l].nbNodesLayer1 + 1,
				network[l].nbNodesLayer2, 1);
	one = ones(network[l].nbNodesLayer1 + 1);
	memmove(z[l] + 1, z[l], network[l].nbNodesLayer1 * sizeof(double));
	z[l][0] = 1.0;
	dif = diff(one, sigmoid(z[l], network[l].nbNodesLayer1 + 1, 1),
				network[l].nbNodesLayer1 + 1, 1);
	eltm1 = eltMult(sigmoid(z[l], network[l].nbNodesLayer1 + 1, 1), dif,
				network[l].nbNodesLayer1 + 1, 1);
	err[l] = eltMult(theta_delta, eltm1, network[l].nbNodesLayer1 + 1, 1) + 1;

	delete[] tr;
	delete[] theta_delta;
	delete[] dif;
	delete[] one;
	delete[] eltm1;
}
\end{minted}
\item Calcul des $\Delta$ :

\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}
for(int l = 0; l < nbLayers - 1; l++){
	tr = trans(a[l],network[l].nbNodesLayer1 + 1, 1);
	theta_delta = mult(err[l+1], tr, network[l].nbNodesLayer2, 1,
					network[l].nbNodesLayer1 + 1);

	for(int k = 0; k < network[l].nbNodesLayer2; k++)
		for(int j = 0; j < network[l].nbNodesLayer1 + 1; j++)
			bigDelta[l][k][j] = bigDelta[l][k][j] + theta_delta[k *
										(network[l].nbNodesLayer1 + 1) + j];
}
\end{minted}
\item Calcul du gradient on utilisant $\Delta$ calculé à l'etape precedente, la valeur $\lambda$ et les poids $\Theta$.
\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}
for(int l=0; l < nbLayers - 1; l++){
   for(int i=0; i < network[l].nbNodesLayer2; i++) {
		 D[l][i][0] = bigDelta[l][i][0]/setSize;
		 for(int j=1; j < network[l].nbNodesLayer1 + 1; j++)
			 D[l][i][j] = bigDelta[l][i][j]/setSize +
			 lambda*network[l].weights[i * (network[l].nbNodesLayer1 + 1) + j];
	}
}
\end{minted}
\end{itemize}
\begin{verbatim}

\end{verbatim}
\boldred{Note} : $l$ est le numero de l'intercouche , $i$ est le numero du noeud de la première couche et $j$ est le numero du noeud de la deuxième couche.

\newpage
\paragraph{L'apprentissage}
\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}
void train()
\end{minted}
La fonction de l'apprentissage du programme.
Pour utiliser les fonctions du minimisation de la bibliothèque \textit{dlib} il faut adapter la structure des vecteurs a celle utilisée dans cette bibliothèque. Cette transformation est trés couteuse elle n'est que temporaire, remplacer les tableau double par des \textit{matrix<double, 0, 1>} est nécéssaire.
Pour faire ca on a créé une fonction qui transforme un vecteur ou matrice représentée sous notre structure à une structure compatible avec les fonctions de \textit{dlib}.

\begin{minted}[linenos=true, tabsize=4, fontfamily=courier, fontsize=\small, xleftmargin=5pt, xrightmargin=5pt]{cpp}
find_min(bfgs_search_strategy(), objective_delta_stop_strategy(1e-7),
	[this](column_vector const&v) -> double { return costFunction(v); },
	[this](column_vector const&v) -> column_vector { return costFunctionDerivative(v); },
	init_theta, -1);
\end{minted}

On utilise pour la minimisation la methode \textit{BFGS}, c'est une méthode numérique utilisée pour résoudre des systèmes d'équations non linéaires dont on ne connaît pas forcément l'expression analytique.
Concernant la condition d'arret nous avions le choix entre une difference de la valeur de la fonction objective entre deux itérations (option choisise), et celle de choisir une valeur minimal du gradient. Le problème avec la deuxième option c'est qu'il est difficile de garantir à l'avance que la condition d'arret sera réalisé.

Nous avons aussi utilisé des fonctions anonymes pour contourner le fait que nos fonctions sont des méthodes (et donc prennent le parametre this) alors que la fonction \textit{find\_min} attend des fonctions statiques.
